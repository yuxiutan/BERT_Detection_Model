#!/usr/bin/env python3
"""
BERT æ¨¡å‹æ¨ç†å¼•æ“
è² è²¬è¼‰å…¥æ¨¡å‹ä¸¦åŸ·è¡Œé æ¸¬
"""

import torch
import torch.nn.functional as F
from transformers import BertTokenizer, BertForSequenceClassification
import logging
from pathlib import Path
from typing import Tuple, Optional
import numpy as np

logger = logging.getLogger(__name__)

class BERTDetectionInference:
    """BERT æª¢æ¸¬æ¨¡å‹æ¨ç†é¡"""
    
    def __init__(self, model_path: Optional[str] = None):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.tokenizer = None
        self.max_length = 512
        
        # è¨­å®šæ¨¡å‹è·¯å¾‘å„ªå…ˆé †åº
        if model_path:
            self.model_path = Path(model_path)
        else:
            # å„ªå…ˆä½¿ç”¨ç•¶å‰æ¨¡å‹ï¼Œå‚™é¸åŸå§‹æ¨¡å‹
            current_model = Path("/app/Model/bert_model_current.pth")
            original_model = Path("/app/Model/bert_model.pth")
            
            if current_model.exists():
                self.model_path = current_model
            elif original_model.exists():
                self.model_path = original_model
            else:
                raise FileNotFoundError("æ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆ")
        
        # è¼‰å…¥æ¨¡å‹
        self._load_model()
        
        logger.info(f"âœ… BERT æ¨ç†å¼•æ“åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {self.model_path}")
    
    def _load_model(self):
        """è¼‰å…¥ BERT æ¨¡å‹å’Œ tokenizer"""
        try:
            logger.info(f"ğŸ“¦ æ­£åœ¨è¼‰å…¥æ¨¡å‹: {self.model_path}")
            
            # è¼‰å…¥ tokenizer
            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
            
            # è¼‰å…¥æ¨¡å‹æ¶æ§‹
            self.model = BertForSequenceClassification.from_pretrained(
                'bert-base-uncased',
                num_labels=2  # äºŒåˆ†é¡ï¼š0-æ­£å¸¸, 1-æ”»æ“Š
            )
            
            # è¼‰å…¥è¨“ç·´å¥½çš„æ¬Šé‡
            if self.model_path.exists():
                checkpoint = torch.load(self.model_path, map_location=self.device)
                
                # è™•ç†ä¸åŒçš„ä¿å­˜æ ¼å¼
                if isinstance(checkpoint, dict):
                    if 'model_state_dict' in checkpoint:
                        self.model.load_state_dict(checkpoint['model_state_dict'])
                    elif 'state_dict' in checkpoint:
                        self.model.load_state_dict(checkpoint['state_dict'])
                    else:
                        self.model.load_state_dict(checkpoint)
                else:
                    self.model.load_state_dict(checkpoint)
                
                logger.info("âœ… æ¨¡å‹æ¬Šé‡è¼‰å…¥æˆåŠŸ")
            else:
                logger.warning("âš ï¸ æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨ï¼Œä½¿ç”¨é è¨“ç·´æ¬Šé‡")
            
            # ç§»åˆ°è¨­å‚™ä¸¦è¨­ç‚ºè©•ä¼°æ¨¡å¼
            self.model.to(self.device)
            self.model.eval()
            
        except Exception as e:
            logger.error(f"è¼‰å…¥æ¨¡å‹å¤±æ•—: {str(e)}")
            raise
    
    def predict(self, text: str) -> Tuple[str, float]:
        """
        å°è¼¸å…¥æ–‡æœ¬é€²è¡Œé æ¸¬
        
        Args:
            text: è¦æª¢æ¸¬çš„æ—¥èªŒæ–‡æœ¬
            
        Returns:
            Tuple[str, float]: (é æ¸¬çµæœ, ä¿¡å¿ƒåº¦)
        """
        try:
            if not self.model or not self.tokenizer:
                raise RuntimeError("æ¨¡å‹æˆ– tokenizer æœªæ­£ç¢ºè¼‰å…¥")
            
            # é è™•ç†æ–‡æœ¬
            inputs = self._preprocess_text(text)
            
            # åŸ·è¡Œæ¨ç†
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits = outputs.logits
                
                # è¨ˆç®—æ¦‚ç‡å’Œé æ¸¬
                probabilities = F.softmax(logits, dim=-1)
                confidence = torch.max(probabilities).item()
                predicted_class = torch.argmax(logits, dim=-1).item()
            
            # å°‡æ•¸å€¼é æ¸¬è½‰æ›ç‚ºæ–‡å­—æ¨™ç±¤
            prediction_label = "æ”»æ“Š" if predicted_class == 1 else "æ­£å¸¸"
            
            logger.debug(f"é æ¸¬çµæœ: {prediction_label}, ä¿¡å¿ƒåº¦: {confidence:.4f}")
            
            return prediction_label, confidence
            
        except Exception as e:
            logger.error(f"é æ¸¬å¤±æ•—: {str(e)}")
            return "éŒ¯èª¤", 0.0
    
    def _preprocess_text(self, text: str) -> dict:
        """é è™•ç†è¼¸å…¥æ–‡æœ¬"""
        try:
            # ç·¨ç¢¼æ–‡æœ¬
            encoding = self.tokenizer.encode_plus(
                text,
                add_special_tokens=True,
                max_length=self.max_length,
                return_token_type_ids=False,
                padding='max_length',
                truncation=True,
                return_attention_mask=True,
                return_tensors='pt'
            )
            
            # ç§»åˆ°è¨­å‚™
            return {
                'input_ids': encoding['input_ids'].to(self.device),
                'attention_mask': encoding['attention_mask'].to(self.device)
            }
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬é è™•ç†å¤±æ•—: {str(e)}")
            raise
    
    def batch_predict(self, texts: list) -> list:
        """
        æ‰¹é‡é æ¸¬
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            list: [(é æ¸¬çµæœ, ä¿¡å¿ƒåº¦), ...]
        """
        try:
            results = []
            
            for text in texts:
                prediction, confidence = self.predict(text)
                results.append((prediction, confidence))
            
            return results
            
        except Exception as e:
            logger.error(f"æ‰¹é‡é æ¸¬å¤±æ•—: {str(e)}")
            return [("éŒ¯èª¤", 0.0)] * len(texts)
    
    def reload_model(self, new_model_path: Optional[str] = None):
        """é‡æ–°è¼‰å…¥æ¨¡å‹ï¼ˆç”¨æ–¼æ¨¡å‹æ›´æ–°å¾Œï¼‰"""
        try:
            logger.info("ğŸ”„ é‡æ–°è¼‰å…¥æ¨¡å‹...")
            
            if new_model_path:
                self.model_path = Path(new_model_path)
            
            # é‡æ–°è¼‰å…¥æ¨¡å‹
            self._load_model()
            
            logger.info("âœ… æ¨¡å‹é‡æ–°è¼‰å…¥å®Œæˆ")
            
        except Exception as e:
            logger.error(f"é‡æ–°è¼‰å…¥æ¨¡å‹å¤±æ•—: {str(e)}")
            raise
    
    def get_model_info(self) -> dict:
        """ç²å–æ¨¡å‹è³‡è¨Š"""
        try:
            info = {
                "model_path": str(self.model_path),
                "device": str(self.device),
                "max_length": self.max_length,
                "model_loaded": self.model is not None,
                "tokenizer_loaded": self.tokenizer is not None
            }
            
            if self.model_path.exists():
                stat = self.model_path.stat()
                info.update({
                    "model_size_mb": round(stat.st_size / (1024 * 1024), 2),
                    "last_modified": stat.st_mtime
                })
            
            return info
            
        except Exception as e:
            logger.error(f"ç²å–æ¨¡å‹è³‡è¨Šå¤±æ•—: {str(e)}")
            return {"error": str(e)}

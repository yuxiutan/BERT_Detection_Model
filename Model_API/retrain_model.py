#!/usr/bin/env python3
"""
æ¨¡å‹é‡è¨“ç·´å™¨
è² è²¬ä½¿ç”¨ä½ä¿¡å¿ƒåº¦è³‡æ–™é€²è¡Œå¢é‡è¨“ç·´
"""

import json
import logging
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from transformers import get_linear_schedule_with_warmup
import numpy as np
from sklearn.metrics import accuracy_score, classification_report
from typing import List, Dict, Any, Tuple
import asyncio
from pathlib import Path

logger = logging.getLogger(__name__)

class RetrainDataset(Dataset):
    """é‡è¨“ç·´è³‡æ–™é›†"""
    
    def __init__(self, data: List[Dict[str, Any]], tokenizer, max_length: int = 512):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        text = str(item.get('text', ''))
        label = int(item.get('label', 0))
        
        # ç·¨ç¢¼æ–‡æœ¬
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            return_token_type_ids=False,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

class ModelRetrainer:
    """æ¨¡å‹é‡è¨“ç·´å™¨"""
    
    def __init__(self, model_path: str, training_data_path: str, output_path: str):
        self.model_path = Path(model_path)
        self.training_data_path = Path(training_data_path)
        self.output_path = Path(output_path)
        
        # è¨“ç·´åƒæ•¸
        self.batch_size = 8
        self.learning_rate = 2e-5
        self.epochs = 3
        self.max_length = 512
        self.warmup_steps = 100
        
        # è¨­å‚™
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"ğŸ–¥ï¸ ä½¿ç”¨è¨­å‚™: {self.device}")
        
        # åˆå§‹åŒ– tokenizer
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    
    async def retrain(self) -> bool:
        """åŸ·è¡Œé‡è¨“ç·´æµç¨‹"""
        try:
            logger.info("ğŸ”¥ é–‹å§‹æ¨¡å‹é‡è¨“ç·´...")
            
            # 1. è¼‰å…¥è¨“ç·´è³‡æ–™
            training_data = self._load_training_data()
            if not training_data:
                logger.error("âŒ æ²’æœ‰è¨“ç·´è³‡æ–™")
                return False
            
            # 2. è¼‰å…¥ç¾æœ‰æ¨¡å‹
            model = self._load_existing_model()
            if model is None:
                logger.error("âŒ è¼‰å…¥æ¨¡å‹å¤±æ•—")
                return False
            
            # 3. æº–å‚™è³‡æ–™è¼‰å…¥å™¨
            train_loader = self._prepare_dataloader(training_data)
            
            # 4. è¨­å®šå„ªåŒ–å™¨å’Œæ’ç¨‹å™¨
            optimizer, scheduler = self._setup_optimizer_scheduler(model, train_loader)
            
            # 5. åŸ·è¡Œè¨“ç·´
            training_success = await self._train_model(model, train_loader, optimizer, scheduler)
            if not training_success:
                return False
            
            # 6. ä¿å­˜æ¨¡å‹
            save_success = self._save_model(model)
            if not save_success:
                return False
            
            logger.info("âœ… æ¨¡å‹é‡è¨“ç·´å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ é‡è¨“ç·´å¤±æ•—: {str(e)}")
            return False
    
    def _load_training_data(self) -> List[Dict[str, Any]]:
        """è¼‰å…¥è¨“ç·´è³‡æ–™"""
        try:
            with open(self.training_data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            logger.info(f"ğŸ“Š è¼‰å…¥ {len(data)} ç­†è¨“ç·´è³‡æ–™")
            return data
            
        except Exception as e:
            logger.error(f"è¼‰å…¥è¨“ç·´è³‡æ–™å¤±æ•—: {str(e)}")
            return []
    
    def _load_existing_model(self) -> BertForSequenceClassification:
        """è¼‰å…¥ç¾æœ‰æ¨¡å‹"""
        try:
            # è¼‰å…¥æ¨¡å‹æ¶æ§‹
            model = BertForSequenceClassification.from_pretrained(
                'bert-base-uncased',
                num_labels=2  # äºŒåˆ†é¡ï¼šæ­£å¸¸/æ”»æ“Š
            )
            
            # è¼‰å…¥æ¬Šé‡
            if self.model_path.exists():
                checkpoint = torch.load(self.model_path, map_location=self.device)
                model.load_state_dict(checkpoint)
                logger.info(f"âœ… å·²è¼‰å…¥ç¾æœ‰æ¨¡å‹æ¬Šé‡: {self.model_path}")
            else:
                logger.warning("âš ï¸ æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨ï¼Œä½¿ç”¨é è¨“ç·´æ¬Šé‡")
            
            model.to(self.device)
            return model
            
        except Exception as e:
            logger.error(f"è¼‰å…¥æ¨¡å‹å¤±æ•—: {str(e)}")
            return None
    
    def _prepare_dataloader(self, training_data: List[Dict[str, Any]]) -> DataLoader:
        """æº–å‚™è³‡æ–™è¼‰å…¥å™¨"""
        dataset = RetrainDataset(training_data, self.tokenizer, self.max_length)
        
        return DataLoader(
            dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=0  # Docker ç’°å¢ƒä¸­é¿å…å¤šé€²ç¨‹å•é¡Œ
        )
    
    def _setup_optimizer_scheduler(self, model: nn.Module, train_loader: DataLoader) -> Tuple[AdamW, Any]:
        """è¨­å®šå„ªåŒ–å™¨å’Œå­¸ç¿’ç‡æ’ç¨‹å™¨"""
        optimizer = AdamW(model.parameters(), lr=self.learning_rate)
        
        total_steps = len(train_loader) * self.epochs
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=self.warmup_steps,
            num_training_steps=total_steps
        )
        
        return optimizer, scheduler
    
    async def _train_model(self, model: nn.Module, train_loader: DataLoader, 
                          optimizer: AdamW, scheduler: Any) -> bool:
        """åŸ·è¡Œæ¨¡å‹è¨“ç·´"""
        try:
            model.train()
            
            for epoch in range(self.epochs):
                logger.info(f"ğŸ“š é–‹å§‹ç¬¬ {epoch + 1}/{self.epochs} è¼ªè¨“ç·´...")
                
                epoch_loss = 0
                predictions = []
                true_labels = []
                
                for batch_idx, batch in enumerate(train_loader):
                    # ç§»åˆ°è¨­å‚™
                    input_ids = batch['input_ids'].to(self.device)
                    attention_mask = batch['attention_mask'].to(self.device)
                    labels = batch['labels'].to(self.device)
                    
                    # å‰å‘å‚³æ’­
                    optimizer.zero_grad()
                    outputs = model(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        labels=labels
                    )
                    
                    loss = outputs.loss
                    epoch_loss += loss.item()
                    
                    # åå‘å‚³æ’­
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                    optimizer.step()
                    scheduler.step()
                    
                    # æ”¶é›†é æ¸¬çµæœç”¨æ–¼è©•ä¼°
                    logits = outputs.logits
                    preds = torch.argmax(logits, dim=-1)
                    predictions.extend(preds.cpu().numpy())
                    true_labels.extend(labels.cpu().numpy())
                    
                    # æ¯10æ‰¹æ¬¡è¨˜éŒ„ä¸€æ¬¡é€²åº¦
                    if (batch_idx + 1) % 10 == 0:
                        logger.info(f"  æ‰¹æ¬¡ {batch_idx + 1}/{len(train_loader)}, æå¤±: {loss.item():.4f}")
                    
                    # è®“å…¶ä»–å”ç¨‹æœ‰æ©ŸæœƒåŸ·è¡Œ
                    if batch_idx % 5 == 0:
                        await asyncio.sleep(0.01)
                
                # è¨ˆç®—è¼ªæ¬¡çµ±è¨ˆ
                avg_loss = epoch_loss / len(train_loader)
                accuracy = accuracy_score(true_labels, predictions)
                
                logger.info(f"ğŸ“Š ç¬¬ {epoch + 1} è¼ªå®Œæˆ - å¹³å‡æå¤±: {avg_loss:.4f}, æº–ç¢ºç‡: {accuracy:.4f}")
            
            logger.info("âœ… æ¨¡å‹è¨“ç·´å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"è¨“ç·´å¤±æ•—: {str(e)}")
            return False
    
    def _save_model(self, model: nn.Module) -> bool:
        """ä¿å­˜è¨“ç·´å®Œæˆçš„æ¨¡å‹"""
        try:
            # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
            self.output_path.parent.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜æ¨¡å‹æ¬Šé‡
            torch.save(model.state_dict(), self.output_path)
            
            # ä¿å­˜æ¨¡å‹è³‡è¨Š
            model_info = {
                "retrain_time": datetime.now(timezone.utc).isoformat(),
                "model_path": str(self.output_path),
                "training_data_path": str(self.training_data_path),
                "epochs": self.epochs,
                "learning_rate": self.learning_rate,
                "batch_size": self.batch_size
            }
            
            info_path = self.output_path.parent / "model_info.json"
            with open(info_path, 'w', encoding='utf-8') as f:
                json.dump(model_info, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {self.output_path}")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å‹å¤±æ•—: {str(e)}")
            return False

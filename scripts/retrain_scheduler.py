import os
import json
import glob
import shutil
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
from utils.wazuh_api import fetch_wazuh_data
from Model_API.inference import run_inference
from Model_API.retrain_model import retrain_model
from Model_API.model_build import load_model

LOW_CONF_FILE = os.environ['LOW_CONFIDENCE_FILE']
WEIGHTS_DIR = os.environ['MODEL_WEIGHTS_DIR']
ORIGINAL_PROCESSED_DATA = os.environ['ORIGINAL_PROCESSED_DATA']
ORIGINAL_REFERENCE_EMBEDDINGS = os.environ['ORIGINAL_REFERENCE_EMBEDDINGS']

os.makedirs(WEIGHTS_DIR, exist_ok=True)
os.makedirs(f"{WEIGHTS_DIR}/backups", exist_ok=True)

def monitor_task():
    """æ¯ 3 åˆ†é˜ç²å–è³‡æ–™ä¸¦æ¨è«–ï¼Œä½ä¿¡å¿ƒåº¦äº‹ä»¶å­˜å…¥æª”æ¡ˆ"""
    events = fetch_wazuh_data()
    if not events:
        return

    model = load_model()  # è¼‰å…¥æœ€æ–°æ¨¡å‹
    low_conf_events = []

    for event in events:
        confidence = run_inference(model, event)  # å‡è¨­è¿”å›ä¿¡å¿ƒåº¦
        if confidence < 0.3:
            low_conf_events.append(event)

    if low_conf_events:
        with open(LOW_CONF_FILE, 'a', encoding='utf-8') as f:
            for event in low_conf_events:
                f.write(json.dumps(event, ensure_ascii=False) + '\n')
        print(f"å„²å­˜ {len(low_conf_events)} ç­†ä½ä¿¡å¿ƒåº¦äº‹ä»¶åˆ° {LOW_CONF_FILE}")

def retrain_task():
    """æ¯å¤© 21:00 ä½¿ç”¨ä½ä¿¡å¿ƒåº¦äº‹ä»¶é‡æ–°è¨“ç·´"""
    if not os.path.exists(LOW_CONF_FILE):
        print("ç„¡ä½ä¿¡å¿ƒåº¦äº‹ä»¶ï¼Œè·³é Retrain")
        return

    with open(LOW_CONF_FILE, 'r', encoding='utf-8') as f:
        low_conf_data = [json.loads(line) for line in f if line.strip()]

    if not low_conf_data:
        print("ä½ä¿¡å¿ƒåº¦äº‹ä»¶æª”æ¡ˆç‚ºç©ºï¼Œè·³é Retrain")
        return

    # å‚™ä»½æœ€æ–°ä¸‰ç‰ˆæ¬Šé‡
    current_processed = sorted(glob.glob(f"{WEIGHTS_DIR}/processed_data_v*.pkl"), key=os.path.getmtime)
    current_embeddings = sorted(glob.glob(f"{WEIGHTS_DIR}/reference_embeddings_v*.pkl"), key=os.path.getmtime)
    
    if len(current_processed) >= 3:
        backups = current_processed[-3:]  # æœ€æ–°ä¸‰ç‰ˆ
        for i, weight in enumerate(backups):
            shutil.copy(weight, f"{WEIGHTS_DIR}/backups/processed_data_backup_v{i+1}.pkl")
            embedding = weight.replace("processed_data", "reference_embeddings")
            if os.path.exists(embedding):
                shutil.copy(embedding, f"{WEIGHTS_DIR}/backups/reference_embeddings_backup_v{i+1}.pkl")
            print(f"å‚™ä»½ {weight} å’Œå°æ‡‰åµŒå…¥æª”æ¡ˆ")

    # é‡æ–°è¨“ç·´
    new_version = len(current_processed) + 1
    new_processed_path = f"{WEIGHTS_DIR}/processed_data_v{new_version}.pkl"
    new_embeddings_path = f"{WEIGHTS_DIR}/reference_embeddings_v{new_version}.pkl"
    
    retrain_model(
        low_conf_data,
        base_processed=ORIGINAL_PROCESSED_DATA,
        base_embeddings=ORIGINAL_REFERENCE_EMBEDDINGS,
        output_processed=new_processed_path,
        output_embeddings=new_embeddings_path
    )
    print(f"ç”Ÿæˆæ–°æ¬Šé‡: {new_processed_path}, {new_embeddings_path}")

    # æ¸…ç©ºä½ä¿¡å¿ƒåº¦äº‹ä»¶æª”æ¡ˆ
    open(LOW_CONF_FILE, 'w').close()
    print("æ¸…ç©ºä½ä¿¡å¿ƒåº¦äº‹ä»¶æª”æ¡ˆ")

def start_scheduler():
    """å•Ÿå‹•æ’ç¨‹å™¨"""
    scheduler = BackgroundScheduler()
    scheduler.add_job(monitor_task, 'interval', minutes=3)
    scheduler.add_job(retrain_task, CronTrigger(hour=21, minute=0))
    scheduler.start()
    print("ğŸ“… æ’ç¨‹å™¨å•Ÿå‹•")
